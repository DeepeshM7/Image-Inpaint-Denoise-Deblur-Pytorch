{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Install PyTorch XLA**\n\nFor using TPU"},{"metadata":{"trusted":true},"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch_xla.core.xla_model as xm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torchvision\nimport numpy as np\nfrom torchvision import transforms\nimport os\nimport glob\nfrom PIL import Image\nimport cv2\nfrom skimage.util import random_noise\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(torch.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformations = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor()\n    \n])\n\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare Data-Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_root = \"../input/flickrfaceshq-dataset-ffhq\"\n\nclass FaceData(torch.utils.data.Dataset):\n    def __init__(self, transformations=None, root = '../input/flickrfaceshq-dataset-ffhq'):\n        self.root = root\n        self.transformations = transformations\n        self.img_list = glob.glob(os.path.join(root,'*'))\n        self.img_list.sort()\n    \n    def __len__(self):\n        return len(glob.glob(\"../input/flickrfaceshq-dataset-ffhq/*\"))\n    \n    def mask(self, img):\n        canvas = np.full((224,224,3), 255, np.uint8) #draw white canvas\n        \n        for _ in range(np.random.randint(1,10)):\n            x1, x2 = np.random.randint(1,224), np.random.randint(1,224)\n            y1, y2 = np.random.randint(1,224), np.random.randint(1,224)\n            t = np.random.randint(1,3)\n            \n            cv2.line(canvas,(x1,y1),(x2,y2),(0,0,0),t)\n            \n            \n        masked_img = img.copy()\n        masked_img[canvas==0] = 255\n            \n        return masked_img, canvas\n        \n    def random_blur(self,img):\n        blur = np.random.choice(['average','gaussian','median'])\n        \n        if blur == 'average':\n            img = cv2.blur(img,(7,7))\n\n        elif blur == 'gaussian':\n            img = cv2.GaussianBlur(img,(9,9),0)\n\n        else:\n            img = cv2.medianBlur(img,7)\n\n        return img\n\n        \n    def __getitem__(self, idx):\n     \n        y_img = cv2.imread(self.img_list[idx]) #Original image\n        y_img = cv2.cvtColor(y_img,cv2.COLOR_BGR2RGB)\n\n        y_pil = Image.fromarray(y_img)\n        y = self.transformations(y_pil)\n        \n        x_img = cv2.imread(self.img_list[idx]) #Image to be transformed        \n        x_img = cv2.cvtColor(x_img,cv2.COLOR_BGR2RGB)\n        x_img = cv2.resize(x_img,(224,224))\n       \n        x_img = self.random_blur(x_img)\n        \n        x,_ = self.mask(x_img)\n        x = Image.fromarray(x)\n        x = self.transformations(x)\n        x = random_noise(x, mode='s&p') #random_noise converts img to float in range[0,1]\n\n        return x, y\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#GET SAMPLE DATA\ndata_set = FaceData(transformations=transformations)\n\nsample_loader = torch.utils.data.DataLoader(data_set, batch_size=32)\n\nbatch=next(iter(sample_loader))\nxx,yy=batch\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Visualize Training Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid1 = torchvision.utils.make_grid(xx,nrow=16)\ngrid2 = torchvision.utils.make_grid(yy,nrow=16)\n\nfig, axs = plt.subplots(2,1,figsize=(50,50),sharex=True)#2 rows, 1 column\n\naxs[0].imshow(np.transpose(grid1,(1,2,0)))\naxs[1].imshow(np.transpose(grid2,(1,2,0)))\n\nplt.subplots_adjust(top=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Define DataLoaders and Split DATA into Train, Validation and Test**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define DataLoaders\nindices = list(range(len(data_set.img_list)))\nval_split = int(0.9*len(data_set.img_list)) \ntest_split = int(val_split+ ((len(data_set.img_list)-val_split)//2))\n\ntrain_indices,val_indices,test_indices = indices[:val_split], indices[val_split:test_split],indices[test_split:]\n\ntrain_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices)\nval_sampler = torch.utils.data.sampler.SubsetRandomSampler(val_indices)\ntest_sampler = torch.utils.data.sampler.SubsetRandomSampler(test_indices)\n\ntrain_data = torch.utils.data.DataLoader(data_set,batch_size =256, sampler=train_sampler,\n                                        num_workers = 8, pin_memory=True)\nval_data = torch.utils.data.DataLoader(data_set,batch_size = 16, sampler =val_sampler,\n                                      num_workers =4, pin_memory=True)\ntest_data = torch.utils.data.DataLoader(data_set,batch_size = 4, sampler =test_sampler,\n                                      num_workers =4, pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(torch.cuda.is_available())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate Mean and Variance:\n\nUsed to train VQ-VAE"},{"metadata":{"trusted":true},"cell_type":"code","source":"##To Get Mean and Variance of Data\n\n# num_pixels = len(train_indices)*224*224*3\n\n# sum_total = 0\n\n# for batch in train_data:\n#     sum_total += batch[1].flatten().sum()\n\n# mean =  sum_total/num_pixels\n\n# sum_sqr_error=0\n# for batch in train_data:\n#     sum_sqr_error += ((batch[1].flatten() - mean).pow(2)).sum()\n    \n# train_variance = sum_sqr_error/num_pixels\n\n# mean, train_variance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = xm.xla_device()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_variance = 0.0838","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **VQ-VAE Code**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#VQ-VAE\n\nclass VectorQuantizer(nn.Module):\n    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n        super(VectorQuantizer, self).__init__()\n        \n        self._embedding_dim = embedding_dim\n        self._num_embeddings = num_embeddings\n        \n        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n        self._commitment_cost = commitment_cost\n\n    def forward(self, inputs):\n        # convert inputs from BCHW -> BHWC\n        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n        input_shape = inputs.shape\n        \n        # Flatten input\n        flat_input = inputs.view(-1, self._embedding_dim)\n        \n        # Calculate distances\n        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n                    + torch.sum(self._embedding.weight**2, dim=1)\n                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n            \n        # Encoding\n        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n        encodings.scatter_(1, encoding_indices, 1)\n        \n        # Quantize and unflatten\n        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n        \n        # Loss\n        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n        \n        quantized = inputs + (quantized - inputs).detach()\n        avg_probs = torch.mean(encodings, dim=0)\n        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n        \n        # convert quantized from BHWC -> BCHW\n        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings\n    \nclass VectorQuantizerEMA(nn.Module):\n    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):\n        super(VectorQuantizerEMA, self).__init__()\n        \n        self._embedding_dim = embedding_dim\n        self._num_embeddings = num_embeddings\n        \n        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n        self._embedding.weight.data.normal_()\n        self._commitment_cost = commitment_cost\n        \n        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n        self._ema_w.data.normal_()\n        \n        self._decay = decay\n        self._epsilon = epsilon\n\n    def forward(self, inputs):\n        # convert inputs from BCHW -> BHWC\n        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n        input_shape = inputs.shape\n        \n        # Flatten input\n        flat_input = inputs.view(-1, self._embedding_dim)\n        \n        # Calculate distances\n        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n                    + torch.sum(self._embedding.weight**2, dim=1)\n                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n            \n        # Encoding\n        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n        encodings.scatter_(1, encoding_indices, 1)\n        \n        # Quantize and unflatten\n        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n        \n        # Use EMA to update the embedding vectors\n        if self.training:\n            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n                                     (1 - self._decay) * torch.sum(encodings, 0)\n            \n            # Laplace smoothing of the cluster size\n            n = torch.sum(self._ema_cluster_size.data)\n            self._ema_cluster_size = (\n                (self._ema_cluster_size + self._epsilon)\n                / (n + self._num_embeddings * self._epsilon) * n)\n            \n            dw = torch.matmul(encodings.t(), flat_input)\n            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n            \n            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n        \n        # Loss\n        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n        loss = self._commitment_cost * e_latent_loss\n        \n        # Straight Through Estimator\n        quantized = inputs + (quantized - inputs).detach()\n        avg_probs = torch.mean(encodings, dim=0)\n        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n        \n        # convert quantized from BHWC -> BCHW\n        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings\n    \nclass Residual(nn.Module):\n    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n        super(Residual, self).__init__()\n        self._block = nn.Sequential(\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=in_channels,\n                      out_channels=num_residual_hiddens,\n                      kernel_size=3, stride=1, padding=1, bias=False),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=num_residual_hiddens,\n                      out_channels=num_hiddens,\n                      kernel_size=1, stride=1, bias=False)\n        )\n    \n    def forward(self, x):\n        return x + self._block(x)\n\n\nclass ResidualStack(nn.Module):\n    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n        super(ResidualStack, self).__init__()\n        self._num_residual_layers = num_residual_layers\n        self._layers = nn.ModuleList([Residual(in_channels, num_hiddens, num_residual_hiddens)\n                             for _ in range(self._num_residual_layers)])\n\n    def forward(self, x):\n        for i in range(self._num_residual_layers):\n            x = self._layers[i](x)\n        return F.relu(x)\n    \nclass Encoder(nn.Module):\n    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n        super(Encoder, self).__init__()\n\n        self._conv_1 = nn.Conv2d(in_channels=in_channels,\n                                 out_channels=num_hiddens//2,\n                                 kernel_size=4,\n                                 stride=2, padding=1)\n        self._conv_2 = nn.Conv2d(in_channels=num_hiddens//2,\n                                 out_channels=num_hiddens,\n                                 kernel_size=4,\n                                 stride=2, padding=1)\n        self._conv_3 = nn.Conv2d(in_channels=num_hiddens,\n                                 out_channels=num_hiddens,\n                                 kernel_size=3,\n                                 stride=1, padding=1)\n        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n                                             num_hiddens=num_hiddens,\n                                             num_residual_layers=num_residual_layers,\n                                             num_residual_hiddens=num_residual_hiddens)\n\n    def forward(self, inputs):\n        x = self._conv_1(inputs)\n        x = F.relu(x)\n        \n        x = self._conv_2(x)\n        x = F.relu(x)\n        \n        x = self._conv_3(x)\n        return self._residual_stack(x)\n    \nclass Decoder(nn.Module):\n    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n        super(Decoder, self).__init__()\n        \n        self._conv_1 = nn.Conv2d(in_channels=in_channels,\n                                 out_channels=num_hiddens,\n                                 kernel_size=3, \n                                 stride=1, padding=1)\n        \n        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n                                             num_hiddens=num_hiddens,\n                                             num_residual_layers=num_residual_layers,\n                                             num_residual_hiddens=num_residual_hiddens)\n        \n        self._conv_trans_1 = nn.ConvTranspose2d(in_channels=num_hiddens, \n                                                out_channels=num_hiddens//2,\n                                                kernel_size=4, \n                                                stride=2, padding=1)\n        \n        self._conv_trans_2 = nn.ConvTranspose2d(in_channels=num_hiddens//2, \n                                                out_channels=3,\n                                                kernel_size=4, \n                                                stride=2, padding=1)\n\n    def forward(self, inputs):\n        x = self._conv_1(inputs)\n        \n        x = self._residual_stack(x)\n        \n        x = self._conv_trans_1(x)\n        x = F.relu(x)\n        \n        return self._conv_trans_2(x)\n\n\nclass Model(nn.Module):\n    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens, \n                 num_embeddings, embedding_dim, commitment_cost, decay=0):\n        super(Model, self).__init__()\n        \n        self._encoder = Encoder(3, num_hiddens,\n                                num_residual_layers, \n                                num_residual_hiddens)\n        self._pre_vq_conv = nn.Conv2d(in_channels=num_hiddens, \n                                      out_channels=embedding_dim,\n                                      kernel_size=1, \n                                      stride=1)\n        if decay > 0.0:\n            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim, \n                                              commitment_cost, decay)\n        else:\n            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,\n                                           commitment_cost)\n        self._decoder = Decoder(embedding_dim,\n                                num_hiddens, \n                                num_residual_layers, \n                                num_residual_hiddens)\n\n    def forward(self, x):\n        z = self._encoder(x)\n        z = self._pre_vq_conv(z)\n        loss, quantized, perplexity, _ = self._vq_vae(z)\n        \n        x_recon = self._decoder(quantized)\n\n        return loss, x_recon, perplexity","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Building ConVolutional Decoder**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class GatedActivation(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        x, y = x.chunk(2, dim=1)\n        return torch.tanh(x) * torch.sigmoid(y)\n    \nclass Inpaint(nn.Module):\n    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens,\n                gated = True):\n        super(Inpaint, self).__init__()\n        \n        if gated:\n            self.gate = GatedActivation()\n            self._conv_1 = nn.Conv2d(in_channels=in_channels,\n                                     out_channels=num_hiddens*2,#add *2\n                                     kernel_size=3, \n                                     stride=1, padding=1)\n\n            self._residual_stack = ResidualStack(in_channels=num_hiddens,\n                                                 num_hiddens=num_hiddens,\n                                                 num_residual_layers=num_residual_layers,\n                                                 num_residual_hiddens=num_residual_hiddens)\n        \n            self._conv_trans_1 = nn.ConvTranspose2d(in_channels=num_hiddens, \n                                                    out_channels=num_hiddens, \n                                                    kernel_size=4, \n                                                    stride=2, padding=1)\n            self.conv2 = nn.Conv2d(in_channels=num_hiddens//2, \n                                                    out_channels=num_hiddens//2,\n                                                    kernel_size=1, \n                                                    stride=1)\n\n            self._conv_trans_2 = nn.ConvTranspose2d(in_channels=num_hiddens//2, \n                                                    out_channels=6, \n                                                    kernel_size=4, \n                                                    stride=2, padding=1)\n            self.conv3 = nn.Conv2d(in_channels=3, \n                                                    out_channels=3,\n                                                    kernel_size=1, \n                                                    stride=1)\n        else:\n            self.gate = GatedActivation()\n            self._conv_1 = nn.Conv2d(in_channels=in_channels,\n                                     out_channels=num_hiddens,\n                                     kernel_size=3, \n                                     stride=1, padding=1)\n\n            self._residual_stack = ResidualStack(in_channels=num_hiddens,\n                                                 num_hiddens=num_hiddens,\n                                                 num_residual_layers=num_residual_layers,\n                                                 num_residual_hiddens=num_residual_hiddens)\n        \n            self._conv_trans_1 = nn.ConvTranspose2d(in_channels=num_hiddens, \n                                                    out_channels=num_hiddens//2, \n                                                    kernel_size=4, \n                                                    stride=2, padding=1)\n            self.conv2 = nn.Conv2d(in_channels=num_hiddens//2, \n                                                    out_channels=num_hiddens//2,\n                                                    kernel_size=1, \n                                                    stride=1)\n\n            self._conv_trans_2 = nn.ConvTranspose2d(in_channels=num_hiddens//2, \n                                                    out_channels=3, \n                                                    kernel_size=4, \n                                                    stride=2, padding=1)\n            self.conv3 = nn.Conv2d(in_channels=3, \n                                                    out_channels=3,\n                                                    kernel_size=1, \n                                                    stride=1)\n\n    def forward(self, inputs, gated):\n        if gated:\n            x = self.gate(self._conv_1(inputs))\n\n            x = (self._residual_stack(x))\n\n            x = self.gate(self._conv_trans_1(x))\n  \n            x = F.relu(self.conv2(x))\n            x = self.gate(self._conv_trans_2(x))\n        #RElu\n        else:\n            x = F.relu(self._conv_1(inputs))\n\n            x = (self._residual_stack(x))\n\n            x = F.relu(self._conv_trans_1(x))\n\n            x = F.relu(self.conv2(x))\n            x = F.relu(self._conv_trans_2(x))\n\n        return self.conv3(x)\n    \n\nclass Inpaint_Model(nn.Module):\n    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens, \n                 num_embeddings, embedding_dim, commitment_cost, decay=0,inpaint = False,\n                activation = 'gated'):\n        super(Inpaint_Model, self).__init__()\n        \n        if activation == 'gated':\n            self._inpaint = Inpaint(embedding_dim,\n                                    num_hiddens, \n                                    num_residual_layers, \n                                    num_residual_hiddens,gated = True)\n            self.gated = True\n        else:\n            self._inpaint = Inpaint(embedding_dim,\n                                    num_hiddens, \n                                    num_residual_layers, \n                                    num_residual_hiddens,gated = False)\n            self.gated = False\n\n    def forward(self, x):\n\n        x_recon = self._inpaint(x, self.gated)\n\n        return x_recon","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Initialize Parameters**"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 256\nnum_training_updates = 15000\n\nnum_hiddens = 128\nnum_residual_hiddens = 32\nnum_residual_layers = 2\n\nembedding_dim = 64\nnum_embeddings = 512\n\ncommitment_cost = 0.25\n\ndecay = 0.99\n\nlearning_rate = 1e-3\n\nmodel = Model(num_hiddens, num_residual_layers, num_residual_hiddens,\n              num_embeddings, embedding_dim, \n              commitment_cost, decay)\nmodel = model.to(device)\n\ninpaint_model = Inpaint_Model(num_hiddens, num_residual_layers, num_residual_hiddens,\n              num_embeddings, embedding_dim, \n              commitment_cost, decay)\ninpaint_model = inpaint_model.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create Directory to store Weights\nos.mkdir('./weights')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Train VQ-VAE**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Resume Train VQ-VAE\n# # chkpt = torch.load(\"../input/e3241/_epoch_41\",map_location= 'cpu')\n\n# # model.load_state_dict(chkpt['model_state_dict'])\n# # optimizer.load_state_dict(chkpt['optimizer_state_dict'])\n# # chkpt_epoch = chkpt['epoch']\n# # loss_recon = chkpt['loss']['recon_error']\n# # loss_perplex = chkpt['loss']['perplexity']\n\n##Train VQ-VAE\n# model.train()\n\n# train_res_recon_error = loss_recon.copy()\n# train_res_perplexity = loss_perplex.copy()\n\n# for i in range(70):#Total=70 epoch\n\n#     for x,_ in train_data:\n\n#         x = x.to(device)                         \n#         optimizer.zero_grad()\n\n              \n#         vq_loss, data_recon, perplexity = model(x)\n#         recon_error = F.mse_loss(data_recon, x) / x_variance \n#         loss = recon_error + vq_loss\n#         loss.backward()\n\n#         optimizer.step()\n        \n#         train_res_recon_error.append(recon_error.item())\n#         train_res_perplexity.append(perplexity.item())\n        \n#         xm.mark_step()\n\n        \n#     if (i+1) % 10 == 0:\n#         print('%d iterations' % (i+1))\n#         print('recon_error: %.3f' % np.mean(train_res_recon_error[-100:]))\n#         print('perplexity: %.3f' % np.mean(train_res_perplexity[-100:]))\n#         print()\n\n#     if (i+1)%5 == 0:\n#         torch.save({'epoch':i,\n#                     'model_state_dict':model.state_dict(),\n#                     'optimizer_state_dict':optimizer.state_dict(),\n#                     'loss':{'recon_error':train_res_recon_error.copy(),'perplexity':train_res_perplexity.copy()},\n#                    },os.path.join('./weights',f'_epoch_{i+41}'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Train Decoder**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#TRAIN DECODER\nchkpt = torch.load(\"../input/656055/_epoch_60\",map_location= device)\n\nmodel.load_state_dict(chkpt['model_state_dict'])\ninpaint_optimizer = torch.optim.Adam(inpaint_model.parameters(), lr=learning_rate)\n\n#Resume Train\n# chkpt_inpaint = torch.load(\"../input/inpaint-71/inpaint_epoch_71\",map_location= device)\n# inpaint_model.load_state_dict(chkpt_inpaint['model_state_dict'])\n# inpaint_optimizer.load_state_dict(chkpt_inpaint['optimizer_state_dict'])\n# loss_recon = chkpt_inpaint['loss']['recon_error']\n\n\n\ndecoder_train_res_recon_error = []#loss_recon.copy()\n\n\n#Train DECODER\nmodel.eval()\ninpaint_model.train()\nfor param in model.parameters():\n    param.requires_grad = False\n\n\nfor i in range(35): #Total:75\n\n    for x,y in train_data:\n        x = x.to(device)                          \n        y = y.to(device)\n        \n        inpaint_optimizer.zero_grad()\n              \n        x1 = model._pre_vq_conv(model._encoder(x))\n        _,z,_,_ = model._vq_vae(x1)\n        \n        data_recon = inpaint_model(z)\n\n        recon_loss = F.mse_loss(data_recon, y) \n        \n        recon_loss.backward()\n\n        inpaint_optimizer.step()\n        \n        decoder_train_res_recon_error.append(recon_loss.item())\n\n        xm.mark_step()\n    \n\n    if (i+1)%3 == 0:\n        torch.save({'epoch':i,\n                    'model_state_dict':inpaint_model.state_dict(),\n                    'optimizer_state_dict':inpaint_optimizer.state_dict(),\n                    'loss':{'recon_error':decoder_train_res_recon_error.copy()},\n                   },os.path.join('./weights',f'inpaint_relu_epoch_{i+1}'))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Visualize Results**"},{"metadata":{"trusted":true},"cell_type":"code","source":"inpaint_model_gated = Inpaint_Model(num_hiddens, num_residual_layers, num_residual_hiddens,\n              num_embeddings, embedding_dim, \n              commitment_cost, decay).to(device)\n\ninpaint_model_relu = Inpaint_Model(num_hiddens, num_residual_layers, num_residual_hiddens,\n              num_embeddings, embedding_dim, \n              commitment_cost, decay, activation = 'relu').to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chkpt = torch.load(\"../input/656055/_epoch_60\",map_location=torch.device('cpu'))\nchkpt_inpaint = torch.load(\"../input/relu-inpaint-3033/inpaint_relu_epoch_33\",map_location=torch.device('cpu'))\nmodel.load_state_dict(chkpt['model_state_dict'])\n\ninpaint_model_relu.load_state_dict(chkpt_inpaint['model_state_dict'])\n\nmodel.eval()\ninpaint_model_relu.eval()\n\n(valid_originals, y) = next(iter(val_data))\nvalid_originals = valid_originals.to(device)\n\nvq_output_eval = model._pre_vq_conv(model._encoder(valid_originals))\n_, valid_quantize, _, _ = model._vq_vae(vq_output_eval)\nvalid_reconstructions = inpaint_model_relu(valid_quantize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check Inpaint Gated Activation\nchkpt = torch.load(\"../input/656055/_epoch_60\",map_location=torch.device('cpu'))\nchkpt_inpaint = torch.load(\"../input/inpaint35/inpaint_epoch_35 (1)\",map_location=torch.device('cpu'))\nmodel.load_state_dict(chkpt['model_state_dict'])\n\ninpaint_model_gated.load_state_dict(chkpt_inpaint['model_state_dict'])\n\nmodel.eval()\ninpaint_model_gated.eval()\n\nvalid_reconstructions_gated = inpaint_model_gated(valid_quantize)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision.utils import make_grid\ndef show(img,title):\n    npimg = img.numpy()\n    _, ax = plt.subplots(figsize=(20,20))\n    fig = ax.imshow(np.transpose(npimg, (1,2,0)))\n    fig.axes.get_xaxis().set_visible(True)\n    fig.axes.get_yaxis().set_visible(True)\n    plt.title(title, fontsize=40)\n    plt.savefig(f\"./{title}\")\n\nshow(make_grid(valid_reconstructions.cpu().data),\"Using Relu Activation\" )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show(make_grid(valid_reconstructions_gated.cpu().data),\"Using Gated Activation\" )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show(make_grid(valid_originals.cpu().data),\"Input to Model\" )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = chkpt_inpaint['loss']['recon_error']\nf = plt.figure(figsize=(20,8))\nax = f.add_subplot(1,2,1)\nax.plot(loss)\nax.set_yscale('log')\nax.set_title('Smoothed NMSE.')\nax.set_xlabel('iteration')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}